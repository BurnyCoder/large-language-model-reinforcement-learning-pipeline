2026-01-13 05:03:29.295241: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 05:03:29.395371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-13 05:03:31.151728: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                 â”‚
â”‚  Training       â”‚
â”‚  Pipeline       â”‚
â”‚  Run ID:        â”‚
â”‚  20260113_0503  â”‚
â”‚  37_no3z |      â”‚
â”‚  Running 1      â”‚
â”‚  algorithm(s):  â”‚
â”‚  grpo           â”‚
â”‚                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯


â”€ System Informaâ€¦ â”€

System Information 
â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Compoâ€¦ â”ƒ Detaiâ€¦ â”ƒ
â”¡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ Platfâ€¦ â”‚ Linuxâ€¦ â”‚
â”‚ Python â”‚ 3.12.3 â”‚
â”‚ Workiâ€¦ â”‚ /homeâ€¦ â”‚
â”‚ Direcâ€¦ â”‚        â”‚
â”‚ Timesâ€¦ â”‚ 2026-â€¦ â”‚
â”‚        â”‚ 05:03â€¦ â”‚
â”‚ GPU 0  â”‚ NVIDIA â”‚
â”‚        â”‚ GeForâ€¦ â”‚
â”‚        â”‚ RTX    â”‚
â”‚        â”‚ 5070   â”‚
â”‚        â”‚ Laptop â”‚
â”‚        â”‚ GPU    â”‚
â”‚        â”‚ (8.0   â”‚
â”‚        â”‚ GB)    â”‚
â”‚ CUDA   â”‚ 12.8   â”‚
â”‚ Versiâ€¦ â”‚        â”‚
â”‚ RAM    â”‚ 15.3   â”‚
â”‚        â”‚ GB     â”‚
â”‚        â”‚ (43.4% â”‚
â”‚        â”‚ used)  â”‚
â”‚ Disk   â”‚ 1006.9 â”‚
â”‚        â”‚ GB     â”‚
â”‚        â”‚ (13.6% â”‚
â”‚        â”‚ used)  â”‚
â”‚ CPU    â”‚ 16     â”‚
â”‚ Cores  â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”€ Pipeline Confiâ€¦ â”€

GRPO
    GRPO Config    
â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Paramâ€¦ â”ƒ Value  â”ƒ
â”¡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ modelâ€¦ â”‚ Qwen/â€¦ â”‚
â”‚ outpuâ€¦ â”‚ runs/â€¦ â”‚
â”‚ datasâ€¦ â”‚ trl-lâ€¦ â”‚
â”‚ datasâ€¦ â”‚ train  â”‚
â”‚ max_sâ€¦ â”‚ None   â”‚
â”‚ per_dâ€¦ â”‚ 2      â”‚
â”‚ gradiâ€¦ â”‚ 4      â”‚
â”‚ max_sâ€¦ â”‚ -1     â”‚
â”‚ gradiâ€¦ â”‚ Yes    â”‚
â”‚ bf16   â”‚ Yes    â”‚
â”‚ use_lâ€¦ â”‚ No     â”‚
â”‚ datalâ€¦ â”‚ Yes    â”‚
â”‚ datalâ€¦ â”‚ 4      â”‚
â”‚ loggiâ€¦ â”‚ 1      â”‚
â”‚ loggiâ€¦ â”‚ steps  â”‚
â”‚ log_lâ€¦ â”‚ info   â”‚
â”‚ reporâ€¦ â”‚ wandb  â”‚
â”‚ save_â€¦ â”‚ 50     â”‚
â”‚ save_â€¦ â”‚ steps  â”‚
â”‚ save_â€¦ â”‚ 12     â”‚
â”‚ cleanâ€¦ â”‚ No     â”‚
â”‚ verboâ€¦ â”‚ Yes    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”€ Training Progrâ€¦ â”€



â”€â”€ GRPO Training â”€â”€

â„¹ Model: 
Qwen/Qwen2.5-0.5B-I
nstruct
â„¹ Dataset: 
trl-lib/DeepMath-10
3K
â„¹ Output: 
runs/20260113_05033
7_no3z/Qwen2.5-0.5B
-GRPO
â„¹ Output directory 
ready: 
runs/20260113_05033
7_no3z/Qwen2.5-0.5B
-GRPO
[05:â€¦ INFO S grpo.â€¦
           t       
           a       
           r       
           t       
           i       
           n       
           g       
                   
           G       
           R       
           P       
           O       
                   
           t       
           r       
           a       
           i       
           n       
           i       
           n       
           g       
                   
           w       
           i       
           t       
           h       
                   
           m       
           o       
           d       
           e       
           l       
           =       
           Q       
           w       
           e       
           n       
           /       
           Q       
           w       
           e       
           n       
           2       
           .       
           5       
           -       
           0       
           .       
           5       
           B       
           -       
           I       
           n       
           s       
           t       
           r       
           u       
           c       
           t       
 GRPO Extra Config 
â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ Parameâ€¦ â”ƒ Value â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ rewardâ€¦ â”‚ None  â”‚
â”‚ num_geâ€¦ â”‚ 4     â”‚
â”‚ max_coâ€¦ â”‚ 256   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

â„¹ Using model name 
directly: 
Qwen/Qwen2.5-0.5B-I
nstruct
Loading dataset: 
trl-lib/DeepMath-10
3K (split: train)

          Dataset Information          
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Property    â”ƒ Value                 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Dataset     â”‚ trl-lib/DeepMath-103K â”‚
â”‚ Samples     â”‚ 97,870                â”‚
â”‚ Columns     â”‚ prompt, solution      â”‚
â”‚ Sample Keys â”‚ prompt, solution      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â„¹ Using accuracy_reward function
â„¹ Adjusting per_device_train_batch_size from 2 to 4 (must be divisible by 
num_generations=4)
â„¹ Creating GRPO trainer...
  num_generations: 4
  max_completion_length: 256
The model is already on multiple devices. Skipping the move to device specified in `args`.
Using auto half precision backend
Starting GRPO training loop...
Note: GRPO generates multiple completions per sample, which may take longer.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
***** Running training *****
  Num examples = 97,870
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 73,401
  Number of trainable parameters = 494,032,768
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: burny (burny-burny) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 9r030kou
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/burny/projects/ml-playground/llmrl/wandb/run-20260113_050348-9r030kou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-moon-16
wandb: â­ï¸ View project at https://wandb.ai/burny-burny/huggingface
wandb: ğŸš€ View run at https://wandb.ai/burny-burny/huggingface/runs/9r030kou

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GRPO Training Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

 Max Steps:        Full epoch                    
 Batch Size:       4                             
 Gradient Accum:   4                             
 Effective Batch:  16                            
 Learning Rate:    1.00e-06                      
 Warmup Steps:     0                             
 Save Strategy:    SaveStrategy.STEPS (every 50) 


â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ GRPO Training Initialized                                                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Max steps: auto â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training Arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                                   TrainingArguments                                   
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Parameter                               â”ƒ Value                                     â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ output_dir                              â”‚ runs/20260113_050337_no3z/Qwen2.5-0.5B-Gâ€¦ â”‚
â”‚ overwrite_output_dir                    â”‚ No                                        â”‚
â”‚ do_train                                â”‚ No                                        â”‚
â”‚ do_eval                                 â”‚ No                                        â”‚
â”‚ do_predict                              â”‚ No                                        â”‚
â”‚ eval_strategy                           â”‚ IntervalStrategy.NO                       â”‚
â”‚ prediction_loss_only                    â”‚ No                                        â”‚
â”‚ per_device_train_batch_size             â”‚ 4                                         â”‚
â”‚ per_device_eval_batch_size              â”‚ 8                                         â”‚
â”‚ per_gpu_train_batch_size                â”‚ None                                      â”‚
â”‚ per_gpu_eval_batch_size                 â”‚ None                                      â”‚
â”‚ gradient_accumulation_steps             â”‚ 4                                         â”‚
â”‚ eval_accumulation_steps                 â”‚ None                                      â”‚
â”‚ eval_delay                              â”‚ 0                                         â”‚
â”‚ torch_empty_cache_steps                 â”‚ None                                      â”‚
â”‚ learning_rate                           â”‚ 1e-06                                     â”‚
â”‚ weight_decay                            â”‚ 0.0                                       â”‚
â”‚ adam_beta1                              â”‚ 0.9                                       â”‚
â”‚ adam_beta2                              â”‚ 0.999                                     â”‚
â”‚ adam_epsilon                            â”‚ 1e-08                                     â”‚
â”‚ max_grad_norm                           â”‚ 1.0                                       â”‚
â”‚ num_train_epochs                        â”‚ 3.0                                       â”‚
â”‚ max_steps                               â”‚ -1                                        â”‚
â”‚ lr_scheduler_type                       â”‚ SchedulerType.LINEAR                      â”‚
â”‚ lr_scheduler_kwargs                     â”‚ None                                      â”‚
â”‚ warmup_ratio                            â”‚ 0.0                                       â”‚
â”‚ warmup_steps                            â”‚ 0                                         â”‚
â”‚ log_level                               â”‚ info                                      â”‚
â”‚ log_level_replica                       â”‚ warning                                   â”‚
â”‚ log_on_each_node                        â”‚ Yes                                       â”‚
â”‚ logging_dir                             â”‚ runs/20260113_050337_no3z/Qwen2.5-0.5B-Gâ€¦ â”‚
â”‚ logging_strategy                        â”‚ IntervalStrategy.STEPS                    â”‚
â”‚ logging_first_step                      â”‚ No                                        â”‚
â”‚ logging_steps                           â”‚ 1                                         â”‚
â”‚ logging_nan_inf_filter                  â”‚ Yes                                       â”‚
â”‚ save_strategy                           â”‚ SaveStrategy.STEPS                        â”‚
â”‚ save_steps                              â”‚ 50                                        â”‚
â”‚ save_total_limit                        â”‚ 12                                        â”‚
â”‚ save_safetensors                        â”‚ Yes                                       â”‚
â”‚ save_on_each_node                       â”‚ No                                        â”‚
â”‚ save_only_model                         â”‚ No                                        â”‚
â”‚ restore_callback_states_from_checkpoint â”‚ No                                        â”‚
â”‚ no_cuda                                 â”‚ No                                        â”‚
â”‚ use_cpu                                 â”‚ No                                        â”‚
â”‚ use_mps_device                          â”‚ No                                        â”‚
â”‚ seed                                    â”‚ 42                                        â”‚
â”‚ data_seed                               â”‚ None                                      â”‚
â”‚ jit_mode_eval                           â”‚ No                                        â”‚
â”‚ bf16                                    â”‚ Yes                                       â”‚
â”‚ fp16                                    â”‚ No                                        â”‚
â”‚ fp16_opt_level                          â”‚ O1                                        â”‚
â”‚ half_precision_backend                  â”‚ auto                                      â”‚
â”‚ bf16_full_eval                          â”‚ No                                        â”‚
â”‚ fp16_full_eval                          â”‚ No                                        â”‚
â”‚ tf32                                    â”‚ None                                      â”‚
â”‚ local_rank                              â”‚ 0                                         â”‚
â”‚ ddp_backend                             â”‚ None                                      â”‚
â”‚ tpu_num_cores                           â”‚ None                                      â”‚
â”‚ tpu_metrics_debug                       â”‚ No                                        â”‚
â”‚ debug                                   â”‚                                           â”‚
â”‚ dataloader_drop_last                    â”‚ No                                        â”‚
â”‚ eval_steps                              â”‚ None                                      â”‚
â”‚ dataloader_num_workers                  â”‚ 4                                         â”‚
â”‚ dataloader_prefetch_factor              â”‚ None                                      â”‚
â”‚ past_index                              â”‚ -1                                        â”‚
â”‚ run_name                                â”‚ None                                      â”‚
â”‚ disable_tqdm                            â”‚ No                                        â”‚
â”‚ remove_unused_columns                   â”‚ No                                        â”‚
â”‚ label_names                             â”‚ None                                      â”‚
â”‚ load_best_model_at_end                  â”‚ No                                        â”‚
â”‚ metric_for_best_model                   â”‚ None                                      â”‚
â”‚ greater_is_better                       â”‚ None                                      â”‚
â”‚ ignore_data_skip                        â”‚ No                                        â”‚
â”‚ fsdp                                    â”‚                                           â”‚
â”‚ fsdp_min_num_params                     â”‚ 0                                         â”‚
â”‚ fsdp_config                             â”‚ {'min_num_params': 0, 'xla': False,       â”‚
â”‚                                         â”‚ 'xla_fsdp_v2': False,                     â”‚
â”‚                                         â”‚ 'xla_fsdp_grad_ckpt': False}              â”‚
â”‚ fsdp_transformer_layer_cls_to_wrap      â”‚ None                                      â”‚
â”‚ accelerator_config                      â”‚ AcceleratorConfig(split_batches=False,    â”‚
â”‚                                         â”‚ dispatch_batches=None, even_batches=True, â”‚
â”‚                                         â”‚ use_seedable_sampler=True,                â”‚
â”‚                                         â”‚ non_blocking=False,                       â”‚
â”‚                                         â”‚ gradient_accumulation_kwargs=None,        â”‚
â”‚                                         â”‚ use_configured_state=False)               â”‚
â”‚ parallelism_config                      â”‚ None                                      â”‚
â”‚ deepspeed                               â”‚ None                                      â”‚
â”‚ label_smoothing_factor                  â”‚ 0.0                                       â”‚
â”‚ optim                                   â”‚ OptimizerNames.ADAMW_TORCH_FUSED          â”‚
â”‚ optim_args                              â”‚ None                                      â”‚
â”‚ adafactor                               â”‚ No                                        â”‚
â”‚ group_by_length                         â”‚ No                                        â”‚
â”‚ length_column_name                      â”‚ length                                    â”‚
â”‚ report_to                               â”‚ wandb                                     â”‚
â”‚ project                                 â”‚ huggingface                               â”‚
â”‚ trackio_space_id                        â”‚ trackio                                   â”‚
â”‚ ddp_find_unused_parameters              â”‚ None                                      â”‚
â”‚ ddp_bucket_cap_mb                       â”‚ None                                      â”‚
â”‚ ddp_broadcast_buffers                   â”‚ None                                      â”‚
â”‚ dataloader_pin_memory                   â”‚ Yes                                       â”‚
â”‚ dataloader_persistent_workers           â”‚ No                                        â”‚
â”‚ skip_memory_metrics                     â”‚ Yes                                       â”‚
â”‚ use_legacy_prediction_loop              â”‚ No                                        â”‚
â”‚ push_to_hub                             â”‚ No                                        â”‚
â”‚ resume_from_checkpoint                  â”‚ None                                      â”‚
â”‚ hub_model_id                            â”‚ None                                      â”‚
â”‚ hub_strategy                            â”‚ HubStrategy.EVERY_SAVE                    â”‚
â”‚ hub_token                               â”‚ None                                      â”‚
â”‚ hub_private_repo                        â”‚ None                                      â”‚
â”‚ hub_always_push                         â”‚ No                                        â”‚
â”‚ hub_revision                            â”‚ None                                      â”‚
â”‚ gradient_checkpointing                  â”‚ Yes                                       â”‚
â”‚ gradient_checkpointing_kwargs           â”‚ None                                      â”‚
â”‚ include_inputs_for_metrics              â”‚ No                                        â”‚
â”‚ include_for_metrics                     â”‚                                           â”‚
â”‚ eval_do_concat_batches                  â”‚ Yes                                       â”‚
â”‚ fp16_backend                            â”‚ auto                                      â”‚
â”‚ push_to_hub_model_id                    â”‚ None                                      â”‚
â”‚ push_to_hub_organization                â”‚ None                                      â”‚
â”‚ push_to_hub_token                       â”‚ None                                      â”‚
â”‚ mp_parameters                           â”‚                                           â”‚
â”‚ auto_find_batch_size                    â”‚ No                                        â”‚
â”‚ full_determinism                        â”‚ No                                        â”‚
â”‚ torchdynamo                             â”‚ None                                      â”‚
â”‚ ray_scope                               â”‚ last                                      â”‚
â”‚ ddp_timeout                             â”‚ 1800                                      â”‚
â”‚ torch_compile                           â”‚ No                                        â”‚
â”‚ torch_compile_backend                   â”‚ None                                      â”‚
â”‚ torch_compile_mode                      â”‚ None                                      â”‚
â”‚ include_tokens_per_second               â”‚ No                                        â”‚
â”‚ include_num_input_tokens_seen           â”‚ no                                        â”‚
â”‚ neftune_noise_alpha                     â”‚ None                                      â”‚
â”‚ optim_target_modules                    â”‚ None                                      â”‚
â”‚ batch_eval_metrics                      â”‚ No                                        â”‚
â”‚ eval_on_start                           â”‚ No                                        â”‚
â”‚ use_liger_kernel                        â”‚ No                                        â”‚
â”‚ liger_kernel_config                     â”‚ None                                      â”‚
â”‚ eval_use_gather_object                  â”‚ No                                        â”‚
â”‚ average_tokens_across_devices           â”‚ Yes                                       â”‚
â”‚ model_init_kwargs                       â”‚ None                                      â”‚
â”‚ disable_dropout                         â”‚ No                                        â”‚
â”‚ cast_lm_head_to_fp32                    â”‚ No                                        â”‚
â”‚ num_generations                         â”‚ 4                                         â”‚
â”‚ num_generations_eval                    â”‚ None                                      â”‚
â”‚ max_completion_length                   â”‚ 256                                       â”‚
â”‚ ds3_gather_for_generation               â”‚ Yes                                       â”‚
â”‚ shuffle_dataset                         â”‚ Yes                                       â”‚
â”‚ generation_batch_size                   â”‚ 16                                        â”‚
â”‚ steps_per_generation                    â”‚ 4                                         â”‚
â”‚ temperature                             â”‚ 1.0                                       â”‚
â”‚ top_p                                   â”‚ 1.0                                       â”‚
â”‚ top_k                                   â”‚ None                                      â”‚
â”‚ min_p                                   â”‚ None                                      â”‚
â”‚ generation_kwargs                       â”‚ None                                      â”‚
â”‚ chat_template_kwargs                    â”‚ None                                      â”‚
â”‚ repetition_penalty                      â”‚ 1.0                                       â”‚
â”‚ use_transformers_paged                  â”‚ No                                        â”‚
â”‚ cache_implementation                    â”‚ None                                      â”‚
â”‚ use_vllm                                â”‚ No                                        â”‚
â”‚ vllm_mode                               â”‚ server                                    â”‚
â”‚ vllm_model_impl                         â”‚ vllm                                      â”‚
â”‚ vllm_enable_sleep_mode                  â”‚ No                                        â”‚
â”‚ vllm_guided_decoding_regex              â”‚ None                                      â”‚
â”‚ vllm_server_base_url                    â”‚ None                                      â”‚
â”‚ vllm_server_host                        â”‚ 0.0.0.0                                   â”‚
â”‚ vllm_server_port                        â”‚ 8000                                      â”‚
â”‚ vllm_server_timeout                     â”‚ 240.0                                     â”‚
â”‚ vllm_gpu_memory_utilization             â”‚ 0.3                                       â”‚
â”‚ vllm_max_model_length                   â”‚ None                                      â”‚
â”‚ vllm_tensor_parallel_size               â”‚ 1                                         â”‚
â”‚ beta                                    â”‚ 0.0                                       â”‚
â”‚ num_iterations                          â”‚ 1                                         â”‚
â”‚ epsilon                                 â”‚ 0.2                                       â”‚
â”‚ delta                                   â”‚ None                                      â”‚
â”‚ epsilon_high                            â”‚ None                                      â”‚
â”‚ sapo_temperature_neg                    â”‚ 1.05                                      â”‚
â”‚ sapo_temperature_pos                    â”‚ 1.0                                       â”‚
â”‚ importance_sampling_level               â”‚ token                                     â”‚
â”‚ reward_weights                          â”‚ None                                      â”‚
â”‚ scale_rewards                           â”‚ group                                     â”‚
â”‚ loss_type                               â”‚ dapo                                      â”‚
â”‚ mask_truncated_completions              â”‚ No                                        â”‚
â”‚ sync_ref_model                          â”‚ No                                        â”‚
â”‚ ref_model_mixup_alpha                   â”‚ 0.6                                       â”‚
â”‚ ref_model_sync_steps                    â”‚ 512                                       â”‚
â”‚ top_entropy_quantile                    â”‚ 1.0                                       â”‚
â”‚ use_liger_loss                          â”‚ None                                      â”‚
â”‚ vllm_importance_sampling_correction     â”‚ Yes                                       â”‚
â”‚ vllm_importance_sampling_mode           â”‚ sequence_mask                             â”‚
â”‚ vllm_importance_sampling_cap            â”‚ 3.0                                       â”‚
â”‚ use_bias_correction_kl                  â”‚ No                                        â”‚
â”‚ log_completions                         â”‚ No                                        â”‚
â”‚ num_completions_to_print                â”‚ None                                      â”‚
â”‚ log_unique_prompts                      â”‚ No                                        â”‚
â”‚ max_prompt_length                       â”‚ None                                      â”‚
â”‚ wandb_log_unique_prompts                â”‚ None                                      â”‚
â”‚ distributed_state                       â”‚ Distributed environment:                  â”‚
â”‚                                         â”‚ DistributedType.NO                        â”‚
â”‚                                         â”‚ Num processes: 1                          â”‚
â”‚                                         â”‚ Process index: 0                          â”‚
â”‚                                         â”‚ Local process index: 0                    â”‚
â”‚                                         â”‚ Device: cuda                              â”‚
â”‚                                         â”‚                                           â”‚
â”‚ deepspeed_plugin                        â”‚ None                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  0%|          | 0/73401 [00:00<?, ?it/s]Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[05:04:19] WARNING  All reward functions returned  grpo_trainer.py:1216
                    None for the following kwargs:                     
                    {'solution': 'Yes', 'prompt':                      
                    [{'content': "Determine                            
                    whether there exists an                            
                    invertible matrix $P$ such                         
                    that for the quadratic form                        
                    $Q(v) = v'Av$, where $v =                          
                    \\begin{pmatrix} x & y & z & w                     
                    \\end{pmatrix}$ and $A =                           
                    \\begin{pmatrix} 1 & 0 & 0 & 0                     
                    \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0                      
                    & 0 & 1 \\\\ 0 & 0 & 1 & 0                         
                    \\end{pmatrix}$, the                               
                    expression $Q(Pv)$ equals $x^2                     
                    + y^2 - zw$. Assume all terms                      
                    are in $\\mathbb{R}$. Provide                      
                    a justification for your                           
                    answer.", 'role': 'user'}],                        
                    'completion': [{'role':                            
                    'assistant', 'content': "To                        
                    determine whether there exists                     
                    an invertible matrix \\( P \\)                     
                    such that for the quadratic                        
                    form \\( Q(v) = v'Av \\), the                      
                    expression \\( Q(Pv) \\)                           
                    equals \\( x^2 + y^2 - zw \\),                     
                    we start by writing down the                       
                    quadratic form \\( Q(v) \\)                        
                    explicitly. Given \\( v =                          
                    \\begin{pmatrix} x \\\\ y \\\\                     
                    z \\\\ w \\end{pmatrix} \\)                        
                    and \\( A = \\begin{pmatrix} 1                     
                    & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0                     
                    \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0                      
                    & 1 & 0 \\end{pmatrix} \\), we                     
                    have:\n\n\[\nQ(v) = v'Av =                         
                    \\begin{pmatrix} 1 & 0 & 0 & 0                     
                    \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0                      
                    & 0 & 1 \\\\ 0 & 0 & 1 & 0                         
                    \\end{pmatrix}                                     
                    \\begin{pmatrix} x \\\\ y \\\\                     
                    z \\\\ w \\end{pmatrix}                            
                    \\begin{pmatrix} 1"}]}                             
                    Please ensure that at least                        
                    one reward function returns a                      
                    valid reward.                                      

  0%|          | 1/73401 [00:29<610:27:11, 29.94s/it]    Step 1: loss: 0.0000 | lr: 1.00e-06 | grad: 0.00 | epoch: 0.00 | 
GPU: 5.5/8.0GB
  Step 1/73,401 (0.0%) loss=0.0000 | lr=1.00e-06 | grad=0.000
  GPU 0: 5.54GB allocated / 7.73GB reserved / 8.0GB total

                                                     

  0%|          | 1/73401 [00:30<610:27:11, 29.94s/it]Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}


  0%|          | 2/73401 [01:44<1145:35:22, 56.19s/it]{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1e-06, 'num_tokens': 5908.0, 'completions/mean_length': 244.0, 'completions/min_length': 104.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 0.875, 'completions/mean_terminated_length': 160.0, 'completions/min_terminated_length': 104.0, 'completions/max_terminated_length': 216.0, 'rewards/accuracy_reward/mean': 0.0, 'rewards/accuracy_reward/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.3464984558522701, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'step_time': 28.702062020001904, 'epoch': 0.0}
    Step 2: loss: 0.0000 | lr: 1.00e-06 | grad: 0.00 | epoch: 0.00 | 
GPU: 5.5/8.0GB
  Step 2/73,401 (0.0%) loss=0.0000 | lr=1.00e-06 | grad=0.000
  GPU 0: 5.54GB allocated / 10.82GB reserved / 8.0GB total

                                                      

  0%|          | 2/73401 [01:44<1145:35:22, 56.19s/it]Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}


  0%|          | 3/73401 [02:55<1287:14:32, 63.14s/it]{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 9.999863762074086e-07, 'num_tokens': 11336.0, 'completions/mean_length': 256.0, 'completions/min_length': 256.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/accuracy_reward/mean': 0.0, 'rewards/accuracy_reward/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'entropy': 0.2802553176879883, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'step_time': 67.00308722399495, 'epoch': 0.0}
    Step 3: loss: 0.0029 | lr: 1.00e-06 | grad: 1.61 | epoch: 0.00 | GPU: 5.5/8.0GB
  Step 3/73,401 (0.0%) loss=0.0029 | lr=1.00e-06 | grad=1.606
  GPU 0: 5.54GB allocated / 10.82GB reserved / 8.0GB total

                                                      

  0%|          | 3/73401 [02:56<1287:14:32, 63.14s/it]Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}


  0%|          | 4/73401 [04:11<1387:43:08, 68.07s/it]{'loss': 0.0029, 'grad_norm': 1.6059833765029907, 'learning_rate': 9.999727524148172e-07, 'num_tokens': 16776.0, 'completions/mean_length': 255.5, 'completions/min_length': 248.0, 'completions/max_length': 256.0, 'completions/clipped_ratio': 0.9375, 'completions/mean_terminated_length': 248.0, 'completions/min_terminated_length': 248.0, 'completions/max_terminated_length': 248.0, 'rewards/accuracy_reward/mean': 0.0625, 'rewards/accuracy_reward/std': 0.25, 'reward': 0.0625, 'reward_std': 0.125, 'frac_reward_zero_std': 0.75, 'entropy': 0.3370436280965805, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'step_time': 65.86933835200034, 'epoch': 0.0}
    Step 4: loss: 0.0000 | lr: 1.00e-06 | grad: 0.00 | epoch: 0.00 | GPU: 5.5/8.0GB
  Step 4/73,401 (0.0%) loss=0.0000 | lr=1.00e-06 | grad=0.000
  GPU 0: 5.54GB allocated / 10.83GB reserved / 8.0GB total

                                                      

  0%|          | 4/73401 [04:11<1387:43:08, 68.07s/it]Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

