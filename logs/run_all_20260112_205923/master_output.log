
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Run All â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                      â”‚
â”‚  LLM RL Training Pipelines                                                                                           â”‚
â”‚                                                                                                                      â”‚
â”‚  Mode: GRPO                                                                                                          â”‚
â”‚  Scripts: qwen2.5_0.5b.py                                                                                            â”‚
â”‚                                                                                                                      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

                                 System Information                                 
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Property          â”ƒ Value                                                        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Platform          â”‚ Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39 â”‚
â”‚ Python            â”‚ 3.12.3                                                       â”‚
â”‚ Working Directory â”‚ /home/burny/projects/ml-playground/llmrl                     â”‚
â”‚ Timestamp         â”‚ 2026-01-12 20:59:23                                          â”‚
â”‚ GPU 0             â”‚ NVIDIA GeForce RTX 5070 Laptop GPU (8.0 GB)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Running qwen2.5_0.5b.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Command: /home/burny/projects/ml-playground/.venv/bin/python3 /home/burny/projects/ml-playground/llmrl/qwen2.5_0.5b.py 
--stage grpo
Working directory: /home/burny/projects/ml-playground/llmrl

2026-01-12 20:59:31.554532: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly 
different numerical results due to floating-point round-off errors from different computation orders. To turn them off, 
set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-12 20:59:31.697895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to 
use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow 
with the appropriate compiler flags.
2026-01-12 20:59:33.800120: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly 
different numerical results due to floating-point round-off errors from different computation orders. To turn them off, 
set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                       
â”‚
â”‚  Training Pipeline                                                                                                    
â”‚
â”‚  Run ID: 20260112_205939_ejb2 | Running 1 algorithm(s): grpo                                                          
â”‚
â”‚                                                                                                                       
â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ System Information 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                                 System Information
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Component         â”ƒ Details                                                      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Platform          â”‚ Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39 â”‚
â”‚ Python            â”‚ 3.12.3                                                       â”‚
â”‚ Working Directory â”‚ /home/burny/projects/ml-playground/llmrl                     â”‚
â”‚ Timestamp         â”‚ 2026-01-12 20:59:39                                          â”‚
â”‚ GPU 0             â”‚ NVIDIA GeForce RTX 5070 Laptop GPU (8.0 GB)                  â”‚
â”‚ CUDA Version      â”‚ 12.8                                                         â”‚
â”‚ RAM               â”‚ 15.3 GB (39.7% used)                                         â”‚
â”‚ Disk              â”‚ 1006.9 GB (13.9% used)                                       â”‚
â”‚ CPU Cores         â”‚ 16                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pipeline Configuration 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GRPO
                                 GRPO Config
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Parameter                   â”ƒ Value                                       â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ model_name                  â”‚ Qwen/Qwen2.5-0.5B-Instruct                  â”‚
â”‚ output_dir                  â”‚ runs/20260112_205939_ejb2/Qwen2.5-0.5B-GRPO â”‚
â”‚ dataset_name                â”‚ trl-lib/DeepMath-103K                       â”‚
â”‚ dataset_split               â”‚ train                                       â”‚
â”‚ max_samples                 â”‚ None                                        â”‚
â”‚ per_device_train_batch_size â”‚ 2                                           â”‚
â”‚ gradient_accumulation_steps â”‚ 4                                           â”‚
â”‚ max_steps                   â”‚ -1                                          â”‚
â”‚ gradient_checkpointing      â”‚ Yes                                         â”‚
â”‚ bf16                        â”‚ Yes                                         â”‚
â”‚ use_liger_kernel            â”‚ Yes                                         â”‚
â”‚ dataloader_pin_memory       â”‚ Yes                                         â”‚
â”‚ dataloader_num_workers      â”‚ 4                                           â”‚
â”‚ logging_steps               â”‚ 1                                           â”‚
â”‚ logging_strategy            â”‚ steps                                       â”‚
â”‚ log_level                   â”‚ info                                        â”‚
â”‚ report_to                   â”‚ wandb                                       â”‚
â”‚ save_steps                  â”‚ 1073                                        â”‚
â”‚ save_strategy               â”‚ steps                                       â”‚
â”‚ save_total_limit            â”‚ 12                                          â”‚
â”‚ clean_output_dir            â”‚ No                                          â”‚
â”‚ verbose                     â”‚ Yes                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training Progress 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€



â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GRPO Training 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â„¹ Model: Qwen/Qwen2.5-0.5B-Instruct
â„¹ Dataset: trl-lib/DeepMath-103K
â„¹ Output: runs/20260112_205939_ejb2/Qwen2.5-0.5B-GRPO
â„¹ Output directory ready: runs/20260112_205939_ejb2/Qwen2.5-0.5B-GRPO
[20:59:40] INFO     Starting GRPO training with model=Qwen/Qwen2.5-0.5B-Instruct                                        
grpo.py:65
        GRPO Extra Config
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ Parameter             â”ƒ Value â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ reward_func           â”‚ None  â”‚
â”‚ num_generations       â”‚ 4     â”‚
â”‚ max_completion_length â”‚ 256   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

â„¹ Using model name directly: Qwen/Qwen2.5-0.5B-Instruct
Loading dataset: trl-lib/DeepMath-103K (split: train)

          Dataset Information
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Property    â”ƒ Value                 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Dataset     â”‚ trl-lib/DeepMath-103K â”‚
â”‚ Samples     â”‚ 97,870                â”‚
â”‚ Columns     â”‚ prompt, solution      â”‚
â”‚ Sample Keys â”‚ prompt, solution      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â„¹ Using accuracy_reward function
â„¹ Adjusting per_device_train_batch_size from 2 to 4 (must be divisible by num_generations=4)
â„¹ Creating GRPO trainer...
  num_generations: 4
  max_completion_length: 256
[21:12:00] INFO     Applying Liger kernels to model instance with model type: qwen2 with kwargs: {}                     
monkey_patch.py:2847
The model is already on multiple devices. Skipping the move to device specified in `args`.
Using auto half precision backend
Starting GRPO training loop...
Note: GRPO generates multiple completions per sample, which may take longer.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and 
generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 
None, 'pad_token_id': 151643}.
***** Running training *****
  Num examples = 97,870
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 73,401
  Number of trainable parameters = 494,032,768
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: burny (burny-burny) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 8irr7hbs
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/burny/projects/ml-playground/llmrl/wandb/run-20260112_211201-8irr7hbs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-cherry-8
wandb: â­ï¸ View project at https://wandb.ai/burny-burny/huggingface
wandb: ğŸš€ View run at https://wandb.ai/burny-burny/huggingface/runs/8irr7hbs

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GRPO Training Started 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

 Max Steps:        Full epoch
 Batch Size:       4
 Gradient Accum:   4
 Effective Batch:  16
 Learning Rate:    1.00e-06
 Warmup Steps:     0
 Save Strategy:    SaveStrategy.STEPS (every 1073)


â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ GRPO Training Initialized                                                                                             
â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Max steps: auto 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training Arguments 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                                                                      TrainingArguments
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Parameter                               â”ƒ Value                                                                       
â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ output_dir                              â”‚ runs/20260112_205939_ejb2/Qwen2.5-0.5B-GRPO                                 
â”‚
â”‚ overwrite_output_dir                    â”‚ No                                                                          
â”‚
â”‚ do_train                                â”‚ No                                                                          
â”‚
â”‚ do_eval                                 â”‚ No                                                                          
â”‚
â”‚ do_predict                              â”‚ No                                                                          
â”‚
â”‚ eval_strategy                           â”‚ IntervalStrategy.NO                                                         
â”‚
â”‚ prediction_loss_only                    â”‚ No                                                                          
â”‚
â”‚ per_device_train_batch_size             â”‚ 4                                                                           
â”‚
â”‚ per_device_eval_batch_size              â”‚ 8                                                                           
â”‚
â”‚ per_gpu_train_batch_size                â”‚ None                                                                        
â”‚
â”‚ per_gpu_eval_batch_size                 â”‚ None                                                                        
â”‚
â”‚ gradient_accumulation_steps             â”‚ 4                                                                           
â”‚
â”‚ eval_accumulation_steps                 â”‚ None                                                                        
â”‚
â”‚ eval_delay                              â”‚ 0                                                                           
â”‚
â”‚ torch_empty_cache_steps                 â”‚ None                                                                        
â”‚
â”‚ learning_rate                           â”‚ 1e-06                                                                       
â”‚
â”‚ weight_decay                            â”‚ 0.0                                                                         
â”‚
â”‚ adam_beta1                              â”‚ 0.9                                                                         
â”‚
â”‚ adam_beta2                              â”‚ 0.999                                                                       
â”‚
â”‚ adam_epsilon                            â”‚ 1e-08                                                                       
â”‚
â”‚ max_grad_norm                           â”‚ 1.0                                                                         
â”‚
â”‚ num_train_epochs                        â”‚ 3.0                                                                         
â”‚
â”‚ max_steps                               â”‚ -1                                                                          
â”‚
â”‚ lr_scheduler_type                       â”‚ SchedulerType.LINEAR                                                        
â”‚
â”‚ lr_scheduler_kwargs                     â”‚ None                                                                        
â”‚
â”‚ warmup_ratio                            â”‚ 0.0                                                                         
â”‚
â”‚ warmup_steps                            â”‚ 0                                                                           
â”‚
â”‚ log_level                               â”‚ info                                                                        
â”‚
â”‚ log_level_replica                       â”‚ warning                                                                     
â”‚
â”‚ log_on_each_node                        â”‚ Yes                                                                         
â”‚
â”‚ logging_dir                             â”‚ runs/20260112_205939_ejb2/Qwen2.5-0.5B-GRPO/logs                            
â”‚
â”‚ logging_strategy                        â”‚ IntervalStrategy.STEPS                                                      
â”‚
â”‚ logging_first_step                      â”‚ No                                                                          
â”‚
â”‚ logging_steps                           â”‚ 1                                                                           
â”‚
â”‚ logging_nan_inf_filter                  â”‚ Yes                                                                         
â”‚
â”‚ save_strategy                           â”‚ SaveStrategy.STEPS                                                          
â”‚
â”‚ save_steps                              â”‚ 1073                                                                        
â”‚
â”‚ save_total_limit                        â”‚ 12                                                                          
â”‚
â”‚ save_safetensors                        â”‚ Yes                                                                         
â”‚
â”‚ save_on_each_node                       â”‚ No                                                                          
â”‚
â”‚ save_only_model                         â”‚ No                                                                          
â”‚
â”‚ restore_callback_states_from_checkpoint â”‚ No                                                                          
â”‚
â”‚ no_cuda                                 â”‚ No                                                                          
â”‚
â”‚ use_cpu                                 â”‚ No                                                                          
â”‚
â”‚ use_mps_device                          â”‚ No                                                                          
â”‚
â”‚ seed                                    â”‚ 42                                                                          
â”‚
â”‚ data_seed                               â”‚ None                                                                        
â”‚
â”‚ jit_mode_eval                           â”‚ No                                                                          
â”‚
â”‚ bf16                                    â”‚ Yes                                                                         
â”‚
â”‚ fp16                                    â”‚ No                                                                          
â”‚
â”‚ fp16_opt_level                          â”‚ O1                                                                          
â”‚
â”‚ half_precision_backend                  â”‚ auto                                                                        
â”‚
â”‚ bf16_full_eval                          â”‚ No                                                                          
â”‚
â”‚ fp16_full_eval                          â”‚ No                                                                          
â”‚
â”‚ tf32                                    â”‚ None                                                                        
â”‚
â”‚ local_rank                              â”‚ 0                                                                           
â”‚
â”‚ ddp_backend                             â”‚ None                                                                        
â”‚
â”‚ tpu_num_cores                           â”‚ None                                                                        
â”‚
â”‚ tpu_metrics_debug                       â”‚ No                                                                          
â”‚
â”‚ debug                                   â”‚                                                                             
â”‚
â”‚ dataloader_drop_last                    â”‚ No                                                                          
â”‚
â”‚ eval_steps                              â”‚ None                                                                        
â”‚
â”‚ dataloader_num_workers                  â”‚ 4                                                                           
â”‚
â”‚ dataloader_prefetch_factor              â”‚ None                                                                        
â”‚
â”‚ past_index                              â”‚ -1                                                                          
â”‚
â”‚ run_name                                â”‚ None                                                                        
â”‚
â”‚ disable_tqdm                            â”‚ No                                                                          
â”‚
â”‚ remove_unused_columns                   â”‚ No                                                                          
â”‚
â”‚ label_names                             â”‚ None                                                                        
â”‚
â”‚ load_best_model_at_end                  â”‚ No                                                                          
â”‚
â”‚ metric_for_best_model                   â”‚ None                                                                        
â”‚
â”‚ greater_is_better                       â”‚ None                                                                        
â”‚
â”‚ ignore_data_skip                        â”‚ No                                                                          
â”‚
â”‚ fsdp                                    â”‚                                                                             
â”‚
â”‚ fsdp_min_num_params                     â”‚ 0                                                                           
â”‚
â”‚ fsdp_config                             â”‚ {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 
'xla_fsdp_grad_ckpt': False}                          â”‚
â”‚ fsdp_transformer_layer_cls_to_wrap      â”‚ None                                                                        
â”‚
â”‚ accelerator_config                      â”‚ AcceleratorConfig(split_batches=False, dispatch_batches=None, 
even_batches=True, use_seedable_sampler=True,     â”‚
â”‚                                         â”‚ non_blocking=False, gradient_accumulation_kwargs=None, 
use_configured_state=False)                              â”‚
â”‚ parallelism_config                      â”‚ None                                                                        
â”‚
â”‚ deepspeed                               â”‚ None                                                                        
â”‚
â”‚ label_smoothing_factor                  â”‚ 0.0                                                                         
â”‚
â”‚ optim                                   â”‚ OptimizerNames.ADAMW_TORCH_FUSED                                            
â”‚
â”‚ optim_args                              â”‚ None                                                                        
â”‚
â”‚ adafactor                               â”‚ No                                                                          
â”‚
â”‚ group_by_length                         â”‚ No                                                                          
â”‚
â”‚ length_column_name                      â”‚ length                                                                      
â”‚
â”‚ report_to                               â”‚ wandb                                                                       
â”‚
â”‚ project                                 â”‚ huggingface                                                                 
â”‚
â”‚ trackio_space_id                        â”‚ trackio                                                                     
â”‚
â”‚ ddp_find_unused_parameters              â”‚ None                                                                        
â”‚
â”‚ ddp_bucket_cap_mb                       â”‚ None                                                                        
â”‚
â”‚ ddp_broadcast_buffers                   â”‚ None                                                                        
â”‚
â”‚ dataloader_pin_memory                   â”‚ Yes                                                                         
â”‚
â”‚ dataloader_persistent_workers           â”‚ No                                                                          
â”‚
â”‚ skip_memory_metrics                     â”‚ Yes                                                                         
â”‚
â”‚ use_legacy_prediction_loop              â”‚ No                                                                          
â”‚
â”‚ push_to_hub                             â”‚ No                                                                          
â”‚
â”‚ resume_from_checkpoint                  â”‚ None                                                                        
â”‚
â”‚ hub_model_id                            â”‚ None                                                                        
â”‚
â”‚ hub_strategy                            â”‚ HubStrategy.EVERY_SAVE                                                      
â”‚
â”‚ hub_token                               â”‚ None                                                                        
â”‚
â”‚ hub_private_repo                        â”‚ None                                                                        
â”‚
â”‚ hub_always_push                         â”‚ No                                                                          
â”‚
â”‚ hub_revision                            â”‚ None                                                                        
â”‚
â”‚ gradient_checkpointing                  â”‚ Yes                                                                         
â”‚
â”‚ gradient_checkpointing_kwargs           â”‚ None                                                                        
â”‚
â”‚ include_inputs_for_metrics              â”‚ No                                                                          
â”‚
â”‚ include_for_metrics                     â”‚                                                                             
â”‚
â”‚ eval_do_concat_batches                  â”‚ Yes                                                                         
â”‚
â”‚ fp16_backend                            â”‚ auto                                                                        
â”‚
â”‚ push_to_hub_model_id                    â”‚ None                                                                        
â”‚
â”‚ push_to_hub_organization                â”‚ None                                                                        
â”‚
â”‚ push_to_hub_token                       â”‚ None                                                                        
â”‚
â”‚ mp_parameters                           â”‚                                                                             
â”‚
â”‚ auto_find_batch_size                    â”‚ No                                                                          
â”‚
â”‚ full_determinism                        â”‚ No                                                                          
â”‚
â”‚ torchdynamo                             â”‚ None                                                                        
â”‚
â”‚ ray_scope                               â”‚ last                                                                        
â”‚
â”‚ ddp_timeout                             â”‚ 1800                                                                        
â”‚
â”‚ torch_compile                           â”‚ No                                                                          
â”‚
â”‚ torch_compile_backend                   â”‚ None                                                                        
â”‚
â”‚ torch_compile_mode                      â”‚ None                                                                        
â”‚
â”‚ include_tokens_per_second               â”‚ No                                                                          
â”‚
â”‚ include_num_input_tokens_seen           â”‚ no                                                                          
â”‚
â”‚ neftune_noise_alpha                     â”‚ None                                                                        
â”‚
â”‚ optim_target_modules                    â”‚ None                                                                        
â”‚
â”‚ batch_eval_metrics                      â”‚ No                                                                          
â”‚
â”‚ eval_on_start                           â”‚ No                                                                          
â”‚
â”‚ use_liger_kernel                        â”‚ Yes                                                                         
â”‚
â”‚ liger_kernel_config                     â”‚ None                                                                        
â”‚
â”‚ eval_use_gather_object                  â”‚ No                                                                          
â”‚
â”‚ average_tokens_across_devices           â”‚ Yes                                                                         
â”‚
â”‚ model_init_kwargs                       â”‚ None                                                                        
â”‚
â”‚ disable_dropout                         â”‚ No                                                                          
â”‚
â”‚ cast_lm_head_to_fp32                    â”‚ No                                                                          
â”‚
â”‚ num_generations                         â”‚ 4                                                                           
â”‚
â”‚ num_generations_eval                    â”‚ None                                                                        
â”‚
â”‚ max_completion_length                   â”‚ 256                                                                         
â”‚
â”‚ ds3_gather_for_generation               â”‚ Yes                                                                         
â”‚
â”‚ shuffle_dataset                         â”‚ Yes                                                                         
â”‚
â”‚ generation_batch_size                   â”‚ 16                                                                          
â”‚
â”‚ steps_per_generation                    â”‚ 4                                                                           
â”‚
â”‚ temperature                             â”‚ 1.0                                                                         
â”‚
â”‚ top_p                                   â”‚ 1.0                                                                         
â”‚
â”‚ top_k                                   â”‚ None                                                                        
â”‚
â”‚ min_p                                   â”‚ None                                                                        
â”‚
â”‚ generation_kwargs                       â”‚ None                                                                        
â”‚
â”‚ chat_template_kwargs                    â”‚ None                                                                        
â”‚
â”‚ repetition_penalty                      â”‚ 1.0                                                                         
â”‚
â”‚ use_transformers_paged                  â”‚ No                                                                          
â”‚
â”‚ cache_implementation                    â”‚ None                                                                        
â”‚
â”‚ use_vllm                                â”‚ No                                                                          
â”‚
â”‚ vllm_mode                               â”‚ server                                                                      
â”‚
â”‚ vllm_model_impl                         â”‚ vllm                                                                        
â”‚
â”‚ vllm_enable_sleep_mode                  â”‚ No                                                                          
â”‚
â”‚ vllm_guided_decoding_regex              â”‚ None                                                                        
â”‚
â”‚ vllm_server_base_url                    â”‚ None                                                                        
â”‚
â”‚ vllm_server_host                        â”‚ 0.0.0.0                                                                     
â”‚
â”‚ vllm_server_port                        â”‚ 8000                                                                        
â”‚
â”‚ vllm_server_timeout                     â”‚ 240.0                                                                       
â”‚
â”‚ vllm_gpu_memory_utilization             â”‚ 0.3                                                                         
â”‚
â”‚ vllm_max_model_length                   â”‚ None                                                                        
â”‚
â”‚ vllm_tensor_parallel_size               â”‚ 1                                                                           
â”‚
â”‚ beta                                    â”‚ 0.0                                                                         
â”‚
â”‚ num_iterations                          â”‚ 1                                                                           
â”‚
â”‚ epsilon                                 â”‚ 0.2                                                                         
â”‚
â”‚ delta                                   â”‚ None                                                                        
â”‚
â”‚ epsilon_high                            â”‚ None                                                                        
â”‚
â”‚ sapo_temperature_neg                    â”‚ 1.05                                                                        
â”‚
â”‚ sapo_temperature_pos                    â”‚ 1.0                                                                         
â”‚
â”‚ importance_sampling_level               â”‚ token                                                                       
â”‚
â”‚ reward_weights                          â”‚ None                                                                        
â”‚
â”‚ scale_rewards                           â”‚ group                                                                       
â”‚
â”‚ loss_type                               â”‚ dapo                                                                        
â”‚
â”‚ mask_truncated_completions              â”‚ No                                                                          
â”‚
â”‚ sync_ref_model                          â”‚ No                                                                          
â”‚
â”‚ ref_model_mixup_alpha                   â”‚ 0.6                                                                         
â”‚
â”‚ ref_model_sync_steps                    â”‚ 512                                                                         
â”‚
â”‚ top_entropy_quantile                    â”‚ 1.0                                                                         
â”‚
â”‚ use_liger_loss                          â”‚ None                                                                        
â”‚
â”‚ vllm_importance_sampling_correction     â”‚ Yes                                                                         
â”‚
â”‚ vllm_importance_sampling_mode           â”‚ sequence_mask                                                               
â”‚
â”‚ vllm_importance_sampling_cap            â”‚ 3.0                                                                         
â”‚
â”‚ use_bias_correction_kl                  â”‚ No                                                                          
â”‚
â”‚ log_completions                         â”‚ No                                                                          
â”‚
â”‚ num_completions_to_print                â”‚ None                                                                        
â”‚
â”‚ log_unique_prompts                      â”‚ No                                                                          
â”‚
â”‚ max_prompt_length                       â”‚ None                                                                        
â”‚
â”‚ wandb_log_unique_prompts                â”‚ None                                                                        
â”‚
â”‚ distributed_state                       â”‚ Distributed environment: DistributedType.NO                                 
â”‚
â”‚                                         â”‚ Num processes: 1                                                            
â”‚
â”‚                                         â”‚ Process index: 0                                                            
â”‚
â”‚                                         â”‚ Local process index: 0                                                      
â”‚
â”‚                                         â”‚ Device: cuda                                                                
â”‚
â”‚                                         â”‚                                                                             
â”‚
â”‚ deepspeed_plugin                        â”‚ None                                                                        
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  0%|          | 0/73401 [00:00<?, ?it/s]Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /home/burny/projects/ml-playground/llmrl/pipeline.py:119 in run_pipeline                         â”‚
â”‚                                                                                                  â”‚
â”‚   116 â”‚   â”‚   â”‚   â”‚   if algo == "grpo" and grpo_extra is not None:                              â”‚
â”‚   117 â”‚   â”‚   â”‚   â”‚   â”‚   train_grpo(config, grpo_extra)                                         â”‚
â”‚   118 â”‚   â”‚   â”‚   â”‚   else:                                                                      â”‚
â”‚ â± 119 â”‚   â”‚   â”‚   â”‚   â”‚   TRAINERS(config)                                                 â”‚
â”‚   120 â”‚   â”‚   â”‚   â”‚   status = "PASSED"                                                          â”‚
â”‚   121 â”‚   â”‚   â”‚   â”‚   print_success(f"{algo.upper()} completed in {format_duration(time.time()   â”‚
â”‚   122 â”‚   â”‚   â”‚   except Exception as e:                                                         â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/llmrl/algorithms/grpo.py:195 in train_grpo                    â”‚
â”‚                                                                                                  â”‚
â”‚   192 â”‚   # Train                                                                                â”‚
â”‚   193 â”‚   console.print("Starting GRPO training loop...")                 â”‚
â”‚   194 â”‚   console.print("Note: GRPO generates multiple completions per sample, which may    â”‚
â”‚ â± 195 â”‚   trainer.train()                                                                        â”‚
â”‚   196 â”‚                                                                                          â”‚
â”‚   197 â”‚   # Finalize                                                                             â”‚
â”‚   198 â”‚   finalize_training(trainer, config.output_dir, "GRPO")                                  â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/transformers/trainer.py:23 â”‚
â”‚ 25 in train                                                                                      â”‚
â”‚                                                                                                  â”‚
â”‚   2322 â”‚   â”‚   â”‚   finally:                                                                      â”‚
â”‚   2323 â”‚   â”‚   â”‚   â”‚   hf_hub_utils.enable_progress_bars()                                       â”‚
â”‚   2324 â”‚   â”‚   else:                                                                             â”‚
â”‚ â± 2325 â”‚   â”‚   â”‚   return inner_training_loop(                                                   â”‚
â”‚   2326 â”‚   â”‚   â”‚   â”‚   args=args,                                                                â”‚
â”‚   2327 â”‚   â”‚   â”‚   â”‚   resume_from_checkpoint=resume_from_checkpoint,                            â”‚
â”‚   2328 â”‚   â”‚   â”‚   â”‚   trial=trial,                                                              â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/transformers/trainer.py:26 â”‚
â”‚ 74 in _inner_training_loop                                                                       â”‚
â”‚                                                                                                  â”‚
â”‚   2671 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   else contextlib.nullcontext                                       â”‚
â”‚   2672 â”‚   â”‚   â”‚   â”‚   â”‚   )                                                                     â”‚
â”‚   2673 â”‚   â”‚   â”‚   â”‚   â”‚   with context():                                                       â”‚
â”‚ â± 2674 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   tr_loss_step = self.training_step(model, inputs, num_items_in_ba  â”‚
â”‚   2675 â”‚   â”‚   â”‚   â”‚   â”‚                                                                         â”‚
â”‚   2676 â”‚   â”‚   â”‚   â”‚   â”‚   if (                                                                  â”‚
â”‚   2677 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   args.logging_nan_inf_filter                                       â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.p â”‚
â”‚ y:1127 in training_step                                                                          â”‚
â”‚                                                                                                  â”‚
â”‚   1124 â”‚                                                                                         â”‚
â”‚   1125 â”‚   def training_step(self, model, inputs, num_items_in_batch):                           â”‚
â”‚   1126 â”‚   â”‚   time_before = time.perf_counter()                                                 â”‚
â”‚ â± 1127 â”‚   â”‚   output = super().training_step(model, inputs, num_items_in_batch)                 â”‚
â”‚   1128 â”‚   â”‚   self._step += 1                                                                   â”‚
â”‚   1129 â”‚   â”‚   time_after = time.perf_counter()                                                  â”‚
â”‚   1130 â”‚   â”‚   self._current_train_step_time += time_after - time_before                         â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/transformers/trainer.py:40 â”‚
â”‚ 14 in training_step                                                                              â”‚
â”‚                                                                                                  â”‚
â”‚   4011 â”‚   â”‚   â”‚   if hasattr(self.optimizer, "train") and callable(self.optimizer.train):       â”‚
â”‚   4012 â”‚   â”‚   â”‚   â”‚   self.optimizer.train()                                                    â”‚
â”‚   4013 â”‚   â”‚   â”‚                                                                                 â”‚
â”‚ â± 4014 â”‚   â”‚   â”‚   inputs = self._prepare_inputs(inputs)                                         â”‚
â”‚   4015 â”‚   â”‚   â”‚   if is_sagemaker_mp_enabled():                                                 â”‚
â”‚   4016 â”‚   â”‚   â”‚   â”‚   loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumul  â”‚
â”‚   4017 â”‚   â”‚   â”‚   â”‚   return loss_mb.reduce_mean().detach().to(self.args.device)                â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/trl/extras/profiling.py:98 â”‚
â”‚ in wrapper                                                                                       â”‚
â”‚                                                                                                  â”‚
â”‚    95 â”‚   @functools.wraps(func)                                                                 â”‚
â”‚    96 â”‚   def wrapper(self, *args, **kwargs):                                                    â”‚
â”‚    97 â”‚   â”‚   with profiling_context(self, func.__name__):                                       â”‚
â”‚ â±  98 â”‚   â”‚   â”‚   return func(self, *args, **kwargs)                                             â”‚
â”‚    99 â”‚                                                                                          â”‚
â”‚   100 â”‚   return wrapper                                                                         â”‚
â”‚   101                                                                                            â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.p â”‚
â”‚ y:1156 in _prepare_inputs                                                                        â”‚
â”‚                                                                                                  â”‚
â”‚   1153 â”‚   â”‚   â”‚   generate_every = self.args.steps_per_generation * self.num_iterations         â”‚
â”‚   1154 â”‚   â”‚   â”‚   if self._step % generate_every == 0 or self._buffered_inputs is None:         â”‚
â”‚   1155 â”‚   â”‚   â”‚   â”‚   # self._buffered_inputs=None can occur when resuming from a checkpoint    â”‚
â”‚ â± 1156 â”‚   â”‚   â”‚   â”‚   generation_batch = self._generate_and_score_completions(generation_batch  â”‚
â”‚   1157 â”‚   â”‚   â”‚   â”‚   generation_batch = split_pixel_values_by_grid(generation_batch)           â”‚
â”‚   1158 â”‚   â”‚   â”‚   â”‚   generation_batch = shuffle_sequence_dict(generation_batch)                â”‚
â”‚   1159 â”‚   â”‚   â”‚   â”‚   generation_batches = split_tensor_dict(generation_batch, self.args.steps  â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.p â”‚
â”‚ y:1937 in _generate_and_score_completions                                                        â”‚
â”‚                                                                                                  â”‚
â”‚   1934 â”‚   â”‚   # Calculate rewards for each reward function. rewards_per_func aggregates reward  â”‚
â”‚   1935 â”‚   â”‚   # important because rewards will be normalized per group, and completions are di  â”‚
â”‚   1936 â”‚   â”‚   # rewards_per_func to extract each process's subset.                              â”‚
â”‚ â± 1937 â”‚   â”‚   rewards_per_func = self._calculate_rewards(inputs, prompts, completions, complet  â”‚
â”‚   1938 â”‚   â”‚                                                                                     â”‚
â”‚   1939 â”‚   â”‚   # Apply weights to each reward function's output and sum                          â”‚
â”‚   1940 â”‚   â”‚   rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).nansu  â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/trl/extras/profiling.py:98 â”‚
â”‚ in wrapper                                                                                       â”‚
â”‚                                                                                                  â”‚
â”‚    95 â”‚   @functools.wraps(func)                                                                 â”‚
â”‚    96 â”‚   def wrapper(self, *args, **kwargs):                                                    â”‚
â”‚    97 â”‚   â”‚   with profiling_context(self, func.__name__):                                       â”‚
â”‚ â±  98 â”‚   â”‚   â”‚   return func(self, *args, **kwargs)                                             â”‚
â”‚    99 â”‚                                                                                          â”‚
â”‚   100 â”‚   return wrapper                                                                         â”‚
â”‚   101                                                                                            â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/trl/trainer/grpo_trainer.p â”‚
â”‚ y:1200 in _calculate_rewards                                                                     â”‚
â”‚                                                                                                  â”‚
â”‚   1197 â”‚   â”‚   â”‚   â”‚   â”‚   with torch.inference_mode():                                          â”‚
â”‚   1198 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:,   â”‚
â”‚   1199 â”‚   â”‚   â”‚   â”‚   else:                                                                     â”‚
â”‚ â± 1200 â”‚   â”‚   â”‚   â”‚   â”‚   output_reward_func = reward_func(                                     â”‚
â”‚   1201 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   prompts=prompts, completions=completions, completion_ids=complet  â”‚
â”‚   1202 â”‚   â”‚   â”‚   â”‚   â”‚   )                                                                     â”‚
â”‚   1203 â”‚   â”‚   â”‚   â”‚   â”‚   # Convert None values to NaN                                          â”‚
â”‚                                                                                                  â”‚
â”‚ /home/burny/projects/ml-playground/.venv/lib/python3.12/site-packages/trl/rewards/accuracy_rewar â”‚
â”‚ ds.py:52 in accuracy_reward                                                                      â”‚
â”‚                                                                                                  â”‚
â”‚    49 â”‚   ```                                                                                    â”‚
â”‚    50 â”‚   """                                                                                    â”‚
â”‚    51 â”‚   if not is_math_verify_available():                                                     â”‚
â”‚ â±  52 â”‚   â”‚   raise ImportError("Please install the `math_verify` package to use accuracy_rewa   â”‚
â”‚    53 â”‚                                                                                          â”‚
â”‚    54 â”‚   contents = [completion[0]["content"] for completion in completions]                    â”‚
â”‚    55 â”‚   rewards = []                                                                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
ImportError: Please install the `math_verify` package to use accuracy_reward
âœ— GRPO failed: Please install the `math_verify` package to use accuracy_reward
âœ— GRPO Training Failed
â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric   â”ƒ Value    â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Duration â”‚ 13m 2.4s â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Running GRPO... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 1/1 0:12:37 0:00:00

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pipeline Results 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


              Pipeline Results
â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Algorithm â”ƒ       Status       â”ƒ Duration â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚ GRPO      â”‚       FAILED       â”‚ 13m 2.4s â”‚
â”‚           â”‚                    â”‚          â”‚
â”‚ TOTAL     â”‚ 0 passed, 1 failed â”‚ 13m 2.4s â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


âœ— 1 algorithm(s) failed, 0 passed
Total pipeline time: 13m 2.4s

Traceback (most recent call last):
  File "/home/burny/projects/ml-playground/llmrl/qwen2.5_0.5b.py", line 103, in <module>
    run_pipeline(filtered_configs)
  File "/home/burny/projects/ml-playground/llmrl/pipeline.py", line 160, in run_pipeline
    raise RuntimeError(f"{failed} algorithm(s) failed")
RuntimeError: 1 algorithm(s) failed
[1;34mwandb[0m:
[1;34mwandb[0m: ğŸš€ View run [33mwarm-cherry-8[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260112_211201-8irr7hbs/logs[0m

âœ— qwen2.5_0.5b.py failed with code 1
Error: qwen2.5_0.5b.py failed with return code 1

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Pipeline Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

               GRPO Pipeline Results                
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Script          â”ƒ       Status       â”ƒ  Duration â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ qwen2.5_0.5b.py â”‚       FAILED       â”‚ 13m 24.5s â”‚
â”‚                 â”‚                    â”‚           â”‚
â”‚ TOTAL           â”‚ 0 passed, 1 failed â”‚ 13m 24.5s â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Failure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                      â”‚
â”‚  1 pipeline(s) failed                                                                                                â”‚
â”‚  0 passed, 1 failed                                                                                                  â”‚
â”‚                                                                                                                      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
