
╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                                                     │
│  Training Pipeline                                                                                                                                                                  │
│  Run ID: 20260111_064423_khr8 | Running 4 algorithm(s): sft, reward, dpo, grpo                                                                                                      │
│                                                                                                                                                                                     │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯


───────────────────────────────────────────────────────────────────────────────── System Information ──────────────────────────────────────────────────────────────────────────────────

                                 System Information                                 
┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Component         ┃ Details                                                      ┃
┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ Platform          │ Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39 │
│ Python            │ 3.12.3                                                       │
│ Working Directory │ /home/burny/projects/ml-playground/llmrl                     │
│ Timestamp         │ 2026-01-11 06:44:23                                          │
│ GPU 0             │ NVIDIA GeForce RTX 5070 Laptop GPU (8.0 GB)                  │
│ CUDA Version      │ 12.8                                                         │
│ RAM               │ 15.3 GB (66.3% used)                                         │
│ Disk              │ 1006.9 GB (8.9% used)                                        │
│ CPU Cores         │ 16                                                           │
└───────────────────┴──────────────────────────────────────────────────────────────┘


─────────────────────────────────────────────────────────────────────────────── Pipeline Configuration ────────────────────────────────────────────────────────────────────────────────

SFT
                                 SFT Config                                 
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Parameter                   ┃ Value                                      ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ model_name                  │ Qwen/Qwen2.5-0.5B                          │
│ output_dir                  │ runs/20260111_064423_khr8/Qwen2.5-0.5B-SFT │
│ dataset_name                │ trl-lib/Capybara                           │
│ dataset_split               │ train                                      │
│ max_samples                 │ None                                       │
│ per_device_train_batch_size │ 8                                          │
│ gradient_accumulation_steps │ 4                                          │
│ max_steps                   │ -1                                         │
│ gradient_checkpointing      │ Yes                                        │
│ bf16                        │ Yes                                        │
│ use_liger_kernel            │ Yes                                        │
│ dataloader_pin_memory       │ Yes                                        │
│ dataloader_num_workers      │ 4                                          │
│ logging_steps               │ 1                                          │
│ logging_strategy            │ steps                                      │
│ log_level                   │ info                                       │
│ report_to                   │ wandb                                      │
│ save_steps                  │ 42                                         │
│ save_strategy               │ steps                                      │
│ save_total_limit            │ 12                                         │
│ clean_output_dir            │ No                                         │
│ verbose                     │ Yes                                        │
└─────────────────────────────┴────────────────────────────────────────────┘

REWARD
                                 REWARD Config                                 
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Parameter                   ┃ Value                                         ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ model_name                  │ Qwen/Qwen2.5-0.5B-Instruct                    │
│ output_dir                  │ runs/20260111_064423_khr8/Qwen2.5-0.5B-Reward │
│ dataset_name                │ trl-lib/ultrafeedback_binarized               │
│ dataset_split               │ train                                         │
│ max_samples                 │ None                                          │
│ per_device_train_batch_size │ 8                                             │
│ gradient_accumulation_steps │ 4                                             │
│ max_steps                   │ -1                                            │
│ gradient_checkpointing      │ Yes                                           │
│ bf16                        │ Yes                                           │
│ use_liger_kernel            │ Yes                                           │
│ dataloader_pin_memory       │ Yes                                           │
│ dataloader_num_workers      │ 4                                             │
│ logging_steps               │ 1                                             │
│ logging_strategy            │ steps                                         │
│ log_level                   │ info                                          │
│ report_to                   │ wandb                                         │
│ save_steps                  │ 156                                           │
│ save_strategy               │ steps                                         │
│ save_total_limit            │ 12                                            │
│ clean_output_dir            │ No                                            │
│ verbose                     │ Yes                                           │
└─────────────────────────────┴───────────────────────────────────────────────┘

DPO
                                 DPO Config                                 
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Parameter                   ┃ Value                                      ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ model_name                  │ Qwen/Qwen2.5-0.5B-Instruct                 │
│ output_dir                  │ runs/20260111_064423_khr8/Qwen2.5-0.5B-DPO │
│ dataset_name                │ trl-lib/ultrafeedback_binarized            │
│ dataset_split               │ train                                      │
│ max_samples                 │ None                                       │
│ per_device_train_batch_size │ 2                                          │
│ gradient_accumulation_steps │ 4                                          │
│ max_steps                   │ -1                                         │
│ gradient_checkpointing      │ Yes                                        │
│ bf16                        │ Yes                                        │
│ use_liger_kernel            │ Yes                                        │
│ dataloader_pin_memory       │ Yes                                        │
│ dataloader_num_workers      │ 4                                          │
│ logging_steps               │ 1                                          │
│ logging_strategy            │ steps                                      │
│ log_level                   │ info                                       │
│ report_to                   │ wandb                                      │
│ save_steps                  │ 625                                        │
│ save_strategy               │ steps                                      │
│ save_total_limit            │ 12                                         │
│ clean_output_dir            │ No                                         │
│ verbose                     │ Yes                                        │
└─────────────────────────────┴────────────────────────────────────────────┘

GRPO
                                 GRPO Config                                 
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Parameter                   ┃ Value                                       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ model_name                  │ Qwen/Qwen2.5-0.5B-Instruct                  │
│ output_dir                  │ runs/20260111_064423_khr8/Qwen2.5-0.5B-GRPO │
│ dataset_name                │ trl-lib/DeepMath-103K                       │
│ dataset_split               │ train                                       │
│ max_samples                 │ None                                        │
│ per_device_train_batch_size │ 2                                           │
│ gradient_accumulation_steps │ 4                                           │
│ max_steps                   │ -1                                          │
│ gradient_checkpointing      │ Yes                                         │
│ bf16                        │ Yes                                         │
│ use_liger_kernel            │ Yes                                         │
│ dataloader_pin_memory       │ Yes                                         │
│ dataloader_num_workers      │ 4                                           │
│ logging_steps               │ 1                                           │
│ logging_strategy            │ steps                                       │
│ log_level                   │ info                                        │
│ report_to                   │ wandb                                       │
│ save_steps                  │ 1073                                        │
│ save_strategy               │ steps                                       │
│ save_total_limit            │ 12                                          │
│ clean_output_dir            │ No                                          │
│ verbose                     │ Yes                                         │
└─────────────────────────────┴─────────────────────────────────────────────┘


────────────────────────────────────────────────────────────────────────────────── Training Progress ──────────────────────────────────────────────────────────────────────────────────



──────────────────────────────────────────────────────────────────────────────────── SFT Training ─────────────────────────────────────────────────────────────────────────────────────

ℹ Model: Qwen/Qwen2.5-0.5B
ℹ Dataset: trl-lib/Capybara
ℹ Output: runs/20260111_064423_khr8/Qwen2.5-0.5B-SFT
ℹ Output directory ready: runs/20260111_064423_khr8/Qwen2.5-0.5B-SFT
[06:44:23] INFO     Starting SFT training with model=Qwen/Qwen2.5-0.5B                                                                                                        sft.py:40
Loading dataset: trl-lib/Capybara (split: train)

             Dataset Information             
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Property    ┃ Value                       ┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ Dataset     │ trl-lib/Capybara            │
│ Samples     │ 15,806                      │
│ Columns     │ source, messages, num_turns │
│ Sample Keys │ source, messages, num_turns │
└─────────────┴─────────────────────────────┘

ℹ Creating SFT trainer...
